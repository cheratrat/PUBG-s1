{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pubgK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiLfRJcpjWrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "import os\n",
        "assert os.system(\"pip install ultimate==2.15.1\")==0\n",
        "\n",
        "from ultimate.mlp import MLP\n",
        "import gc, sys\n",
        "gc.enable()\n",
        "\n",
        "NUM = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqlPkY434xRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def state(message, start = True, time =0):\n",
        "  if(start):\n",
        "    print('Working on {message}...')\n",
        "  else:\n",
        "    print('Working on {message} took ({round(time, 3)}) Sec \\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJOAWTJQHMik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    #start_mem = df.memory_usage().sum() / 1024**2\n",
        "    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "    #end_mem = df.memory_usage().sum() / 1024**2\n",
        "    #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQgCVFfg28mR",
        "colab_type": "code",
        "outputId": "83ca5285-e9d6-42e7-e9a8-c9261d9e3389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovAuC5tJDAOP",
        "colab_type": "code",
        "outputId": "141ecf8f-b0a3-4db0-ec00-b7eaf80d37a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/Drive/My Drive/Colab Notebooks/pubg"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Drive/My Drive/Colab Notebooks/pubg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP0c0x0TPifN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIR=\"/content/Drive/My Drive/Colab Notebooks/pubg\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCZSvEw2uCH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def reload():\n",
        "  print(\"Building dataframe...\")\n",
        "  gc.collect()\n",
        "  df = reduce_mem_usage(pd.read_csv(INPUT_DIR + '/tt/train_V2.csv')) # <=========== Just a function to reduce memory usage\n",
        "\n",
        "  # Only take the samples with matches that have more than 1 player \n",
        "  # there are matches with no players or just one player ( those samples could affect our model badly) \n",
        "  df = df[df['maxPlace'] > 1]\n",
        "  invalid_match_ids = df[df['winPlacePerc'].isna()]['matchId'].values\n",
        "  df = df[-df['matchId'].isin(invalid_match_ids)]\n",
        "  print(\"Done loading train to dataframe...\")\n",
        "  return df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdV3GZlIuLCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(df, test_size=0.1):\n",
        "    match_ids = df['matchId'].unique().tolist()\n",
        "    train_size = int(len(match_ids) * (1 - test_size))\n",
        "    train_match_ids = random.sample(match_ids, train_size)\n",
        "\n",
        "    train = df[df['matchId'].isin(train_match_ids)]\n",
        "    test = df[-df['matchId'].isin(train_match_ids)]\n",
        "    \n",
        "    return train, test\n",
        "  \n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# Feature Selectors\n",
        "def run_lightgbmreg(df, do_shap):\n",
        "  print(\"LightGBM: Start Light Gradient Boosted Regression...\")\n",
        "\n",
        "  target = 'winPlacePerc'\n",
        "  cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', target]\n",
        "  cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n",
        "  train, val = train_test_split(df, 0.1)\n",
        "\n",
        "  params = {\n",
        "      'n_estimators': 100,\n",
        "      'learning_rate': 0.3, \n",
        "      'num_leaves': 20,\n",
        "      'objective': 'regression_l2', \n",
        "      'metric': 'mae',\n",
        "      'verbose': -1,\n",
        "  }\n",
        "\n",
        "  model = LGBMRegressor(**params)\n",
        "  model.fit(\n",
        "      train[cols_to_fit], train[target],\n",
        "      eval_metric='mae',\n",
        "      verbose=20,\n",
        "  )\n",
        "  \n",
        "  y_true = val[target]\n",
        "  y_pred = model.predict(val[cols_to_fit])\n",
        "  \n",
        "  if do_shap:\n",
        "    show_shap_analysis(model, cols_to_fit, val)\n",
        "  else:\n",
        "    print(\"LightGBM: Selecting features\")\n",
        "    feature_importance = pd.DataFrame(sorted(zip(model.feature_importances_, cols_to_fit), reverse=True), columns=['Value','Feature'])\n",
        "    print(feature_importance)\n",
        "  \n",
        "  return mean_absolute_error(y_true, y_pred)\n",
        "  \n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "def run_selectkbest(df, do_shap):\n",
        "  print(\"SelectKBest: Start SelectKBest...\")\n",
        "\n",
        "  target = 'winPlacePerc'\n",
        "  cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', target]\n",
        "  cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n",
        "  train, val = train_test_split(df, 0.1)\n",
        "\n",
        "  params = {\n",
        "      'score_func': f_classif,\n",
        "      'k': all\n",
        "  }\n",
        "  \n",
        "  model = SelectKBest(**params)\n",
        "  model.fit(\n",
        "      train[cols_to_fit], train[target]\n",
        "  )\n",
        "  \n",
        "  y_true = val[target]\n",
        "  y_pred = model.predict(val[cols_to_fit])\n",
        "  \n",
        "  if do_shap:\n",
        "    show_shap_analysis(model, cols_to_fit, val)\n",
        "  else:\n",
        "    print(\"SelectKBest: Selecting features\")\n",
        "    dfscores = pd.DataFrame(model.scores_)\n",
        "    dfcolumns = pd.DataFrame(train[cols_to_fit].columns)\n",
        "    #concat two dataframes for better visualization \n",
        "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
        "    featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
        "    print(featureScores.nlargest(10,'Score'))  #print 10 best features\n",
        "\n",
        "  return mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "  \n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "def run_randomforest(df, do_shap):\n",
        "  print(\"RandomForestRegressor: Start Random Forest Regression...\")\n",
        "\n",
        "  target = 'winPlacePerc'\n",
        "  cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', target]\n",
        "  cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n",
        "  train, val = train_test_split(df, 0.1)\n",
        "\n",
        "  model = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features='sqrt',\n",
        "                          n_jobs=-1, verbose = 2)\n",
        "  model.fit(\n",
        "      train[cols_to_fit], train[target]\n",
        "  )\n",
        "  \n",
        "  y_true = val[target]\n",
        "  y_pred = model.predict(val[cols_to_fit])\n",
        "  \n",
        "  if do_shap:\n",
        "    show_shap_analysis(model, cols_to_fit, val)\n",
        "  else:\n",
        "    print(\"RandomForestRegressor: Selecting features\")\n",
        "    feature_importance = pd.DataFrame(sorted(zip(model.feature_importances_, cols_to_fit), reverse=True), columns=['Value','Feature'])\n",
        "    print(feature_importance)\n",
        "    \n",
        "  return mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "  \n",
        "from sklearn.linear_model import LassoCV\n",
        "def run_lasso(df, do_shap):\n",
        "  print(\"Lasso CV: Start Lasso Regularization for selecting features...\")\n",
        "\n",
        "  target = 'winPlacePerc'\n",
        "  cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', target]\n",
        "  cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n",
        "  train, val = train_test_split(df, 0.1)\n",
        "\n",
        "  model = LassoCV()\n",
        "  model.fit(train[cols_to_fit], train[target])\n",
        "  \n",
        "  y_true = val[target]\n",
        "  y_pred = model.predict(val[cols_to_fit])\n",
        "  \n",
        "  if do_shap:\n",
        "    show_shap_analysis(model, cols_to_fit, val)\n",
        "  \n",
        "  else:\n",
        "    print(\"Best alpha using built-in LassoCV: %f\" % model.alpha_)\n",
        "    print(\"Best score using built-in LassoCV: %f\" % model.score(train[cols_to_fit], train[target]))\n",
        "    coef = pd.Series(model.coef_, index = train[cols_to_fit].columns)\n",
        "\n",
        "    print(\"LassoCV: Selecting features\")\n",
        "    print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
        "\n",
        "    imp_coef = coef.sort_values()\n",
        "    plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
        "    imp_coef.plot(kind = \"barh\")\n",
        "    plt.title(\"Feature importance using Lasso Model\")\n",
        "  \n",
        "  return mean_absolute_error(y_true, y_pred)\n",
        "  \n",
        "  \n",
        "from xgboost import XGBRegressor\n",
        "def run_xgboost(df, do_shap):\n",
        "  print(\"XGBoost: Start XGBoost Regression...\")\n",
        "\n",
        "  target = 'winPlacePerc'\n",
        "  cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', target]\n",
        "  cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n",
        "  train, val = train_test_split(df, 0.1)\n",
        "\n",
        "  params = {\n",
        "      'n_estimators': 40,\n",
        "      'learning_rate': 0.1, \n",
        "      'num_leaves': 20,\n",
        "      'objective': 'binary:logistic', \n",
        "      'metric': 'mae',\n",
        "      'verbose': 20,\n",
        "      'seed' : 42\n",
        "  }\n",
        "\n",
        "  model = XGBRegressor(**params)\n",
        "  model.fit(\n",
        "      train[cols_to_fit], train[target],\n",
        "      eval_set=[(val[cols_to_fit], val[target])],\n",
        "      eval_metric='mae',\n",
        "      verbose=20,\n",
        "  )\n",
        "  \n",
        "  print(\"XGBoost: Selecting features\")\n",
        "  feature_importance = pd.DataFrame(sorted(zip(model.feature_importances_, cols_to_fit), reverse=True), columns=['Value','Feature'])\n",
        "  print(feature_importance)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZNHFU8mHDub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lightgbm as lgb\n",
        "def run_lgb2(df, do_shap):\n",
        "  print(\"LightGBM: Start Light Gradient Boosted Regression...\")\n",
        "\n",
        "  target = 'winPlacePerc'\n",
        "  cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', target]\n",
        "  cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n",
        "  train, val = train_test_split(df, 0.1)\n",
        "\n",
        "  params = {\"objective\" : \"regression\", \"metric\" : \"mae\", 'n_estimators':1000,\n",
        "              \"num_leaves\" : 31, \"learning_rate\" : 0.5, \"bagging_fraction\" : 0.7,\n",
        "               \"bagging_seed\" : 0, \"num_threads\" : 4,\"colsample_bytree\" : 0.7\n",
        "             }\n",
        "\n",
        "  model = LGBMRegressor(**params)\n",
        "  model.fit(\n",
        "      train[cols_to_fit], train[target],\n",
        "      eval_metric='mae',\n",
        "      verbose=20,\n",
        "  )\n",
        "  \n",
        "  y_true = val[target]\n",
        "  y_pred = model.predict(val[cols_to_fit])\n",
        "  \n",
        "  if do_shap:\n",
        "    show_shap_analysis(model, cols_to_fit, val)\n",
        "  else:\n",
        "    print(\"LightGBM: Selecting features\")\n",
        "    feature_importance = pd.DataFrame(sorted(zip(model.feature_importances_, cols_to_fit), reverse=True), columns=['Value','Feature'])\n",
        "    print(feature_importance)\n",
        "  \n",
        "  return mean_absolute_error(y_true, y_pred)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ouoDW9-NYPi",
        "colab_type": "text"
      },
      "source": [
        "params = {\n",
        "      'n_estimators': 100,\n",
        "      'learning_rate': 0.3, \n",
        "      'num_leaves': 20,\n",
        "      'objective': 'regression_l2', \n",
        "      'metric': 'mae',\n",
        "      'verbose': -1,\n",
        "  }\n",
        "\n",
        "  model = LGBMRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HLaRRwOubG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_experiment(preprocess):\n",
        "    df = reload()\n",
        "    df.drop(columns=['matchType'], inplace=True)\n",
        "    \n",
        "    df = preprocess(df)\n",
        "\n",
        "    #score = run_lightgbmreg(df, False)\n",
        "    score = run_lgb2(df, False)\n",
        "    #score=run_selectkbest(df, False)\n",
        "    #score=run_xgboost(df, False)\n",
        "    #score = feature_engineering(True)\n",
        "    return score\n",
        "\n",
        "def run_experiments(preprocesses):\n",
        "    results = []\n",
        "    for preprocess in preprocesses:\n",
        "        start = time.time()\n",
        "        score = run_experiment(preprocess)\n",
        "        execution_time = time.time() - start\n",
        "        results.append({\n",
        "            'name': preprocess.__name__,\n",
        "            'score': score,\n",
        "            'execution time': f'{round(execution_time, 2)}s'\n",
        "        })\n",
        "        gc.collect()\n",
        "        \n",
        "    return pd.DataFrame(results, columns=['name', 'score', 'execution time']).sort_values(by='score')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnegvXTyugEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def original(df):\n",
        "    return df\n",
        "  #preprocessing functions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWdkVgDDulJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_train_test_set(df, split):\n",
        "  print(\"Generating train and test set...\")\n",
        "  df.drop(columns=['matchType'], inplace=True)\n",
        "  \n",
        "  cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType']\n",
        "  cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n",
        "  train, val = train_test_split(df, split)\n",
        "  \n",
        "  return train[cols_to_fit], val[cols_to_fit]\n",
        "\n",
        "def load_test():\n",
        "  print(\"Building dataframe...\")\n",
        "  df = reduce_mem_usage(pd.read_csv(INPUT_DIR + '/tt/test_V2.csv')) # <=========== Just a function to reduce memory usage\n",
        "\n",
        "  cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType']\n",
        "  cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n",
        "  print(\"Done loading train to dataframe...\")\n",
        "  return df[cols_to_fit]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJxhK5asumqo",
        "colab_type": "code",
        "outputId": "682bbc63-39ae-4ad6-fbf2-369958969215",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.regularizers import l2\n",
        "\n",
        "# adjusted because im still testing so i reduced hidden units from 64 to 32, epoch reduced\n",
        "class NNModel():\n",
        "  # network parameters\n",
        "  batch_size = 32\n",
        "  hidden_units = 32\n",
        "  dropout = 0.1\n",
        "  kernel_regularizer = l2(0.0001)\n",
        "  leaky_relu = (5 ** 0.5 - 3) / 2\n",
        "  \n",
        "  def __init__(self, input_size):\n",
        "    # Regression has 1 output layer\n",
        "    output_shape = 1\n",
        "    \n",
        "    self.model = model = Sequential()\n",
        "    self.model.add(Dense(self.hidden_units, input_dim=input_size))\n",
        "    # self.model.add(LeakyReLU(alpha=self.leaky_relu))\n",
        "    self.model.add(Activation('sigmoid'))\n",
        "    self.model.add(Dropout(self.dropout))\n",
        "    self.model.add(Dense(self.hidden_units))\n",
        "    self.model.add(Activation('sigmoid'))\n",
        "    # self.model.add(LeakyReLU(alpha=self.leaky_relu))\n",
        "    self.model.add(Dropout(self.dropout))\n",
        "    self.model.add(Dense(output_shape))\n",
        "    # this is the output for one-hot vector\n",
        "    self.model.add(Activation('linear'))\n",
        "    \n",
        "  def _summarize(self):\n",
        "    self.model.summary()\n",
        "  \n",
        "  def _compile(self):\n",
        "    self.model.compile(loss='mse',\n",
        "              optimizer='adam',\n",
        "              metrics=['mse', 'mae'])\n",
        "    \n",
        "  def _train(self, x_train, y_train, epochs):\n",
        "    self.model.fit(x_train, y_train, \n",
        "              epochs=epochs, batch_size=self.batch_size)\n",
        "    \n",
        "  def _evaluate(self, x_test, y_test):\n",
        "    return self.model.evaluate(x_test, y_test, batch_size=self.batch_size)\n",
        "  \n",
        "  def _predict(self, x_test):\n",
        "    return self.model.predict(x_test)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qD-T8dIusuz",
        "colab_type": "code",
        "outputId": "ae62eb11-f1be-47bd-a9ae-454c5f14a738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "# Put Feature Selection Experiments here\n",
        "experiment_scores = run_experiments([original]) # original dataframe\n",
        "\n",
        "\n",
        "# Print Scores\n",
        "print(experiment_scores)\n",
        "# Best combination should be used for training and testing on NN"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building dataframe...\n",
            "Done loading train to dataframe...\n",
            "LightGBM: Start Light Gradient Boosted Regression...\n",
            "LightGBM: Selecting features\n",
            "    Value          Feature\n",
            "0    3741        killPlace\n",
            "1    3454    matchDuration\n",
            "2    3314     walkDistance\n",
            "3    2746         maxPlace\n",
            "4    2526        numGroups\n",
            "5    1939      longestKill\n",
            "6    1782      damageDealt\n",
            "7    1296     rideDistance\n",
            "8    1120       rankPoints\n",
            "9    1110  weaponsAcquired\n",
            "10    965            kills\n",
            "11    899        winPoints\n",
            "12    892       killPoints\n",
            "13    871            heals\n",
            "14    794           boosts\n",
            "15    625            DBNOs\n",
            "16    511          assists\n",
            "17    361     swimDistance\n",
            "18    341      killStreaks\n",
            "19    308    headshotKills\n",
            "20    225          revives\n",
            "21    123        teamKills\n",
            "22     36        roadKills\n",
            "23     21  vehicleDestroys\n",
            "       name    score execution time\n",
            "0  original  0.05834        168.92s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E3zpAmp1rMX",
        "colab_type": "code",
        "outputId": "ad29cd58-8113-4398-f56c-f2866244923c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training Phase\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "df = reload()\n",
        "target = 'winPlacePerc'\n",
        "\n",
        "print(\"Num of rows of csv is \", len(df.index))\n",
        "\n",
        "train, val = generate_train_test_set(df, 0.1)\n",
        "x_train, y_train = train.drop(target, axis=1).to_numpy(), train[target].to_numpy()\n",
        "x_eval, y_eval = val.drop(target, axis=1).to_numpy(), val[target].to_numpy()\n",
        "\n",
        "scaler = StandardScaler().fit(x_train)\n",
        "rescaled_x_train = scaler.transform(x_train)\n",
        "\n",
        "del x_train\n",
        "del train\n",
        "del val\n",
        "gc.collect()\n",
        "\n",
        "num_labels = len(x_eval[0])\n",
        "model1 = NNModel(num_labels)\n",
        "model1._summarize()\n",
        "model1._compile()\n",
        "model1._train(rescaled_x_train, y_train, 2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building dataframe...\n",
            "Done loading train to dataframe...\n",
            "Num of rows of csv is  4446965\n",
            "Generating train and test set...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 32)                800       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 33        \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 1,889\n",
            "Trainable params: 1,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/2\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "4000701/4000701 [==============================] - 480s 120us/step - loss: 0.0108 - mean_squared_error: 0.0108 - mean_absolute_error: 0.0754\n",
            "Epoch 2/2\n",
            "4000701/4000701 [==============================] - 475s 119us/step - loss: 0.0092 - mean_squared_error: 0.0092 - mean_absolute_error: 0.0704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO_wjXa2WF_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a5b430f0-af7c-4c8c-90a5-c2187f2f4ae4"
      },
      "source": [
        "# Training Evaluation\n",
        "# USE THIS FOR DEBUGGING PURPOSES: final training should have all training dataset for accuracy\n",
        "rescaled_x_test = scaler.transform(x_eval)\n",
        "score = model1._evaluate(rescaled_x_test, y_eval)\n",
        "print(\"\\nMean Squared Error [smaller the better]: %f\" % (score[1]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "446264/446264 [==============================] - 17s 37us/step\n",
            "\n",
            "Mean Squared Error [smaller the better]: 0.007771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiFk5OxUWLEh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "b28ac782-9dc4-4c72-a850-7616893ad144"
      },
      "source": [
        "# testing phase\n",
        "# https://www.kaggle.com/ceshine/a-simple-post-processing-trick-lb-0237-0204/\n",
        "def transform_preds(df_test, pred):\n",
        "  for i in range(len(df_test)):\n",
        "      winPlacePerc_m = pred[i]\n",
        "      maxPlace = int(df_test.iloc[i]['maxPlace'])\n",
        "      if maxPlace == 0:\n",
        "          winPlacePerc_m = 0.0\n",
        "      elif maxPlace == 1:\n",
        "          winPlacePerc_m = 1.0\n",
        "      else:\n",
        "          gap = 1.0 / (maxPlace - 1)\n",
        "          winPlacePerc_m = np.round(winPlacePerc_m / gap) * gap\n",
        "\n",
        "      if winPlacePerc_m < 0: winPlacePerc_m = 0.0\n",
        "      if winPlacePerc_m > 1: winPlacePerc_m = 1.0    \n",
        "      pred[i] = winPlacePerc_m\n",
        "\n",
        "      if (i + 1) % 100000 == 0:\n",
        "          print(i, flush=True, end=\" \")\n",
        "\n",
        "  df_test['winPlacePerc_mod'] = pred\n",
        "  return df_test\n",
        "\n",
        "df_test = load_test() # load test data from csv\n",
        "x_test = df_test.to_numpy() # df to numpy array\n",
        "scaled_x_test = scaler.transform(x_test) # scaling\n",
        "\n",
        "del x_test\n",
        "gc.collect()\n",
        "pred = model1._predict(scaled_x_test)\n",
        "df_test['orig_preds'] = pred\n",
        "df_test = transform_preds(df_test['maxPlace', 'orig_preds'], pred)\n",
        "# Sample Table\n",
        "df_test[:5]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building dataframe...\n",
            "Done loading train to dataframe...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ('maxPlace', 'orig_preds')",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-39db9af34bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_x_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'orig_preds'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maxPlace'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'orig_preds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m# Sample Table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ('maxPlace', 'orig_preds')"
          ]
        }
      ]
    }
  ]
}